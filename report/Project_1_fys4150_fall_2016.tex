\documentclass[10pt]{article}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{flexisym}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}

\begin{document}
\setlength\parindent{1pt}
\title{Project 1}
\author{Andrei Kukharenka and Anna Gribkovskaya \\  
FYS 4150 
}

\maketitle
\begin{abstract}
In this work we solve the one-dimensional Poisson equation with Dirichlet boundary conditions by rewriting it a set of linear equations. The latter was solved by some linear algebra methods: Gaussian elimination and LU decomposition. Both methods are applicable for such task, but differ in CPU time. Numerical precision is the same. Solution converges, except for the big matrix sizes where the convergence is destroyed by the round-off effects.
\end{abstract}
\clearpage 


\section{Introduction}
In this project we are dealing with the Poisson equation. This is a second order inhomogeneous differential equation of elliptic type and we deal with its simple case in one dimension. We discretize this equation and rewrite it in form of system of linear equations and use some methods from linear algebra to find a numerical solution. This kind of equations is common in physics. For example it can describe an electrostatic or gravitational fields.  \\* 
Below is a brief description of the project structure. \\* 
In this project we take a look at Poisson equation for electrostatic potential generated by a localized charge distribution. The mathematical details are discussed in Part 1. After we formulated the problem we consider some general and most useful methods we use to solve it in the Part 2. We are going to use linear algebra methods for the problem. A short description of the Gaussian elimination and LU decomposition will be given there, as well as some discussion of the pros and cons for each of them.\\* 
In Part 3. the results are presented. We took the most important data and present it with critical discussion of algorithms. The last part of the report is Conclusion where we sum up all results and come up with the possible topics for the further studies  in this topic. 

\section{Problem formulation}
The problem we deal with is a second order differential equation (DE) with inhomogenious term. Generally it can be written in a following form:
\[
\frac{d^2y}{dx^2}+k^2(x)y = f(x),
\]
where $y$ is unknown function of $x$, $f(x)$ is inhomogeneous term and $k^2$ is a real function.\\* 
Our DE is a classical equation of electrostatic potential, which is generated by known charge distribution. 
\[
\nabla^2 \Phi = -4\pi \rho ({\bf r}).
\]
where $\Phi$ is electrostatic potential and $\rho ({\bf r})$ is a charge distribution. \\* 
This is equation for three dimensions. We simplify this to one dimension equation in $r$ assuming the functions for electrostatic potential and charge  being spherically symmetric. After some simple variable substitution we get a general form for one-dimensional Poisson equation. For our case we use a Dirichlet boundary conditions for this equation. It looks as follows:
\begin{equation}\label{equ:one}
-u''(x) = f(x), \hspace{0.5cm} x\in(0,1), \hspace{0.3cm} u(0) = u(1) = 0.
\end{equation}

Here we introduce a disctretization of the derivative. In order to do it we discretize a domain and define grid points for the $ x $. We have $x\rightarrow x_{i}$ which corresponds to discretized value of a function in this point $u(x_{i})\rightarrow v_i$. For simplisity we have the same distance between all grid point and this is our step lenght $ h $. Each grid poin can now be calculated as $x_i=ih$. The valuse $ h $ can be calculated as $h=1/(n+1)$. And the boundary condition is now defined as $v_0 = v_{n+1} = 0$ where $ n $ is a number of steps. The discretized Poisson equation then will be
\[
   -\frac{v_{i+1}+v_{i-1}-2v_i}{h^2} = f_i  \hspace{0.5cm} \mathrm{for} \hspace{0.1cm} i=1,\dots, n,
\]

 $f_i$ is function value at grid point $i$ \-- $f(x_i)$. \\* 
 It's easy to show that our discretized equation can be written as a set of linear algebra equations (SLAE):
 \\* 
  \\* 
\begin{equation}
\begin{cases}
0 - v_{1}-v_{2} = f_1h^2 \\ -v_{1}+2v_{2}-v_{3} = f_1h^2  \\ 
-v_{2}+2v_{3}-v_{4} = f_1h^2  \\ \dots  \\ -v_{n-1}+2v_{n}-0= f_1h^2 \\
\end{cases},
\end{equation}

here we use $v_0 = v_{n+1} = 0$ conditions and miultiply the right hand side of the equation with an $ h^2 $. To simplify this we now introduce a new function $\tilde{b}_i=h^2f_i$. This system can be now written in a matrix form as:
\begin{equation}\label{equ:three}
   {\bf A}{\bf v} = \tilde{{\bf b}},
\end{equation}
where ${\bf A}$ is an $n\times n$  tridiagonal matrix  
\begin{equation}
    {\bf A} = \left(\begin{array}{cccccc}
                           2& -1& 0 &\dots   & \dots &0 \\
                           -1 & 2 & -1 &0 &\dots &\dots \\
                           0&-1 &2 & -1 & 0 & \dots \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           0&\dots   &  &-1 &2& -1 \\
                           0&\dots    &  & 0  &-1 & 2 \\
                      \end{array} \right),
\end{equation}
In our case we have a tridiagonal matrix with the same elements along the main diagonal (as well as to other diagonals). However we will look at more general case, the tridiagonal matrix with different elements along diagonals. 
\begin{equation}
    {\bf A} = \left(\begin{array}{cccccc}
                           b_1& c_1 & 0 &\dots   & \dots &\dots \\
                           a_1 & b_2 & c_2 &\dots &\dots &\dots \\
                           & a_2 & b_3 & c_3 & \dots & \dots \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           &   &  &a_{n-2}  &b_{n-1}& c_{n-1} \\
                           &    &  &   &a_{n-1} & b_n \\
                      \end{array} \right)\left(\begin{array}{c}
                           v_1\\
                           v_2\\
                           \dots \\
                          \dots  \\
                          \dots \\
                           v_n\\
                      \end{array} \right)
  =\left(\begin{array}{c}
                           \tilde{b}_1\\
                           \tilde{b}_2\\
                           \dots \\
                           \dots \\
                          \dots \\
                           \tilde{b}_n\\
                      \end{array} \right).
\end{equation}
This kind of SLAE can be solved using different linear algebra methods. In the next section we will discuss some of them.
\section{Theory and methods}
\subsection{Gaussian elimination (GE)} 
When we have a SLAE formulated as in (3), we would like to change the matrix $ A $ so it become all zeroes below the diagonal. The most straightforward way to do it is Gauss elimination. \\*
The algorithm for the Gaussian elimination can be described as to main operations the forward substitution and the backward substitution. In the first part, the forward substitution, we would transform the original matrix $ A $ to some other matrix, with zeroes below the main diagonal. This can be done using some operations with matrix rows. We are using the first row to get rid of the elements below the diagonal in the first column. This operation however will change the other matrix elements. After this we are using the second row to eliminate all the elements below the diagonal in the second column and so on. Gaussian elimination require $ n^{3} $ FLOPS. In our case we are lucky to have tridiagonal matrix, so the only elements to be changed are the elements along the diagonal. So in our algorithm we can treat $ A $ as a three vectors (or arrays), not as a square matrix $ n\times n $. This will make our life much easier and the algorithm will be faster.
The new matrix will look as follows 

\begin{equation}
    {\bf A} = \left(\begin{array}{cccccc}
                             b_1& c_1 & 0 &\dots   & \dots &\dots \\
                           0 & {b_2}\textprime=b_2-\frac{a_1}{b_1} & c_2 &\dots &\dots &\dots \\
                           & 0 & {b_3}\textprime=b_3-\frac{a_2}{b_2} & c_3 & \dots & \dots \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           &   &  &0  &\dots & c_{n-1} \\
                           &    &  &   &0 & {b_n}\textprime=b_{n}-\frac{a_{n-1}}{b_{n-1}} \\
                      \end{array} \right),
\end{equation}
After this we also need to change the right hand side part of the SLAE, the vector $ \tilde{b} $. It will now be written as follows
\begin{equation}
\left(\begin{array}{c}
                           \tilde{b}_1\\
                           {\tilde{b}_2}\textprime=\tilde{b}_2-\tilde{b}_1\frac{a_1}{b_1}\\
                           \dots \\
                           \dots \\
                          \dots \\
                           {\tilde{b}_n}\textprime=\tilde{b}_n - \tilde{b}_{n-1}\frac{a_{n-1}}{b_{n-1}}\\
                      \end{array} \right).
\end{equation}
What we have done so far is so called forward substitution. The GE consist of two big parts: the forward substitution (FS) and the backward substitution(BS). After we done the FS we should perform the BS to find the unknown vector $ v $ form (3). \\
We start with finding $ v_{n} $ and go up to $ v_{1} $
\begin{equation}
\begin{cases}
v_{n}=  \frac{{\tilde{b}\textprime_n}}{{b}\textprime_n}\\ 
v_{n-1}=  \frac{{\tilde{b}\textprime_{n-1}-c_{n-1}v_{n}}}{{b}\textprime_{n-1}}  \\ 
\dots   \\ 
\dots  \\ 
v_{1}= \frac{{\tilde{b}\textprime_{1}-c_{1}v_{2}}}{{b}\textprime_{1}} \\
\end{cases},
\end{equation}

This is brute force algorithm for GE in case of tridiagonal matrix (sometimes called Thomas algorithm). This arlorithm require $ 8n-1 $ floating points operations (FLOPS). In the program we will, however test two algorithms. The second one is adjusted to our case where we have same elements along the diagonals, so we can just pre-calculate some coefficients. We call this impimentation the optimized GE and it is easy to show that such optimization reduces the number of FLOPS to $ 6n-5 $. In the results section we will go back to it and compare the time needed to implement the algorithms in a adjusted and in a brute force way. Code section with implementation of algorithm can be found at \url{https://github.com/andrei-fys/fys4150_project1/blob/master/gauss.cpp#L64:L77}.

\subsection{LU decomposition}
This is another way to solve (3) using the linear algebra methods. In this case we will look for a way to write a matrix $ A $ in a following form
\begin{equation}
A=LU
\end{equation}
here $ L $ is lower triangular matrix and $ U $ is upper triangular matrix. It's important that $ L $ to have only $ 1 $ on the main diagonal. Easy to show that $ \det A $ in this case is equal to $ \det U $. In order to implement this method we are going to use one of the libraries in C++, so I will not derive the algorithm here. For the detailed mathematical description please refer to \cite{one}. The detailed programming implimentation is discussed in \cite{two}.
The most important thing here is that we will need the whole matrix $ A $, not the the arrays that represent the diagonal as we can do it in the GE. The number of FLOPS for such algorithm is proportional to $ n^{3} $.

\newpage
\section{Results and discussion}

In this part we present results obtained by Gaussian elimination and LU decomposition numerical methods. We compare efficiency of these two methods and make rough estimation for the convergence rate.
Table \ref{tab:one} represents time measurement results for three algorithms: the brute force GE for tridiagonal matrices, the optimized GE for tridiagonal matrices and LU decomposition. As we expect the slowest algorithm is LU decomposition while both GE algorithms give slightly different result of computation time. One can see that CPU time correlates with number of floating point operations discussed in section 2.
Both GE algorithms have the same factor of ten, that corresponds to what we have predicted from the theoretical approach. Number of floating point operations for these two methods differ just a bit. On the other hand it is difficult to analyze LU decomposition algorithm CPU time and CPU time for GE as LU in our particular case has much more RAM read-write operations which add extra-time. We used three vectors in our GE algorithm realization while for LU we initialized whole matrix of size $n \times n$. It leads to significant RAM requirements for big matrices regarding LU decomposition. We were unable to use LU decomposition for matrix of size $10^5 \times 10^5$ on our computer with just 8 GB of RAM while it could be possible with 80 GB of RAM. However a strong advantage of LU decomposition method is that right-hand sides are not needed to be known in advance as in GE method.
In our particular case LU decomposition is rather overkill and not optimal usage of computer resources nevertheless it can be used for a smaller values of $N$ to compare results of our GE algorithm as both methods have same numerical precision.

\begin{table}
  \caption{CPU time for Gaussian elimination(both brute force and optimized) and LU decomposition for tridiagonal matrices of N-dimensionality. Calculations were performed on Intel architecture CPU i7-3615QM.}
  \label{tab:one}

  \begin{center}
    \begin{tabular}{c|c|c|c}
    \hline
        N & Gaussian el., $s$ & Gaussian el., optimized, $s$ & LU decomposition, $s$ \\
        \hline
        $10$ & $3 \times 10^{-6}$ & $1 \times 10^{-6}$ & $1.9 \times 10^{-5}$ \\
        $10^2$ & $5 \times 10^{-6}$ & $5 \times 10^{-6}$ & $2.7 \times 10^{-3}$\\
        $10^3$ & $5.2 \times 10^{-5}$ & $4.7 \times 10^{-5}$ & $1.8$\\
        $10^4$ & $3.2 \times 10^{-4}$ & $2.5 \times 10^{-4}$ & $2.6 \times 10^{3}$\\
        $10^5$ & $2.8 \times 10^{-3}$ & $2.7 \times 10^{-3}$ & -\\
    \end{tabular}
  \end{center}
\end{table}
On figure \ref{fig:gauss100} numerical($V_i$) and exact($U_i$) solutions plotted for $100$ grid points. One can see computed values fits quite good exact solution. Relative error is no larger than $0.82\%$ for $100$ grid points. Relative error decreases with slope $~2.2$ with step size decrease. Deceasing trend continues till $10^5$ grid points as it shown on figure \ref{fig:error}. With a number of grid points larger then $10^5$ one can see domination of round-off error becomes stronger.
\begin{figure}
  \begin{center}
    \includegraphics[scale=0.7]{100x100}
    \caption{Numerical($V_i$) and exact($U_i$) solutions plotted for $100$ grid points. The numerical solution was obtained by Gauss elimination algorithm. To show the difference between numerical and exact solution at given grid size we represent magnified(12 times) part of a graph.}
    \label{fig:gauss100}
  \end{center}
\end{figure}

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.7]{relative_error_log}
    \caption{Dependency between maximum relative error and domain discretization step size on logarithmic scale. Roughly estimation of convergence rate is $2.2$. Round-off errors dominate for the small step size (less then $10^{-5}$).}
    \label{fig:error}
  \end{center}
\end{figure}
\newpage
\section{Conclusion and further research}
In this project we apply the linear algebra methods for the SLAE obtained after discretization of Poisson equation. It turns out that both Gaussian Elimination and LU decomposition gives us the same numerical precision. However, the LU decomposition takes much more CPU time. At the same time the problem we solved was quite simple, for more complicated tasks, for example, finding the inverse of the matrix LU should be a much better tool to be used.

\newpage
\begin{thebibliography}{9}
\bibitem{one} 
Morten Hjorth-Jensen. 
\textit{Computational Physics
}. 
Lecture Notes Fall 2015, August 2015.
 W. Press, B. Flannery, S. Teukolsky, W. Vetterling, Numerical Recipes in C++, The art of scientific Computing (Cambridge University Press, 1999)

\bibitem{two} 
W. Press, B. Flannery, S. Teukolsky, W. Vetterling 
\textit{Numerical Recipes in C++, The art of scientific Computing}. 
Cambridge University Press, 1999.
 
\end{thebibliography}

\end{document}
